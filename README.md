# ASAG of Computerized Exams

Automatic Short Answer Grading of a College Course's Computerized Exams: PhD Course Project.

## Repo Contents

This repository includes:

1. `extract_exam_data.sh`, a shell script that takes the directory of PDF exam questionnaires and solutions, and outputs CSV (comma-separated value) files containing structured data (questions ID and text, correct answer text, and student answers) that will be fed into the neural network (python program.)

2. `main.py`, the Python program that takes the CSV files and applies the **SFRN+** model (taken from [https://github.com/psunlpgroup/ASAG/tree/main](https://github.com/psunlpgroup/ASAG/tree/main)) in an attempt to automatically grade the student answers. After performing the training and testing, the program prints the evaluation results using 3 statistics: accuracy, f1 score, and qwk (Quadratic Weighted Kappa) using the Cohen Kappa Score. `main.py` relies on the following modules, which are also in this repository:

   a. `Constants.py`: defines the paths to the directories with training and testing CSV files, hyperparameters for the SFRN+ model, and code that reads the question and correct answer data from one of the generated CSV files.

   b. `DataModule.py`: reads the student answers from the training and testing CSV files, and inputs this information into the tensor.

   c. `SFRNModel.py`: defines the SFRN+ model.

   d. `utils.py`: includes a utility called `sort_batch()`.

3. `requirements.txt` listing the required modules that must be installed for the Python program to run.

4. `exams`, a directory containing PDF exam and solution sheets, divided into `train` and `test` folders. The `extract_exam_data.sh` script will take the paths to these folders to extract data from these files into formatted CSV files.

5. `csv_data_files`, a directory containing the CSV files that were generated by `extract_exam_data.sh` and on which we ran experiments.

6. `experiments`, a directory containing outputs generated from the Python program while running experiments.

## Running the `extract_exam_data.sh` script:

### Install Prerequisite software:

The script requires the installation of:

1. `pdftk` ([https://www.pdflabs.com/tools/pdftk-server/](https://www.pdflabs.com/tools/pdftk-server/)), and
2. `pdftotext` ([https://www.xpdfreader.com/download.html](https://www.xpdfreader.com/download.html)) Most Linux/UNIX distros should have this command already pre-installed.

Once installed, make sure that each of the programs is on the `PATH` environmental variable (so that the script can access it.) Otherwise, you may place the command-line executables of the programs in the same directory of this project, and change the shell script's code to search for these programs in the current directory.

To grant yourself executing permission to run the `bash` shell script, type:

    chmod u+x extract_exam_data.sh

Then, to run the script on MSYS2 or Cygwin productions or Linux/UNIX computers, type:

    ./extract_exam_data.sh

`bash` version 4 and up should be used; the experiments were run in `GNU bash, version 5.2.15(1)-release (x86_64-pc-msys)`. The version of the installed `sed` utility is `sed (GNU sed) 4.9`, and the version of the installed `awk` utility is `GNU Awk 5.2.2, API 3.2, PMA Avon 8-g1, (GNU MPFR 4.2.0-p9, GNU MP 6.2.1)`.

Once the script runs, it will ask for two inputs before doing its job: the paths to the PDF files pertaining to the training and testing. If you are running the script from inside the project's main directory, you should enter the paths:

    ./exams/train

and

    ./exams/test

The script will, afterward, proceed to create the CSV files (which might take around 30 minutes for the amount of provided PDF files in this repository, or otherwise longer if the number of PDF files you use is larger.)

Throughout its execution, the script creates several folders and files in which it stores extracted data in various formats (the script must pass the data through several "cleaning" stages) before creating its final product: the CSV files, which will be stored in a directory called `csv_data_files`, which the script will generate.

## Running the Python program to automatically grade exams:

### Prerequisites

The Python version used to run the experiment is: `3.6.8`.

You can also create an environment in `conda`, if preferred:

    conda create -n newenv python=3.6.8 

    conda activate newenv

Then install the requirements:

    pip install -r requirements.txt 

    python -m spacy download en

### Download pre-trained word vectors

The model uses pre-trained word vectors. 

Download the Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download) from [https://www.kaggle.com/datasets/anmolkumar/glove-embeddings?select=glove.6B.100d.txt](https://www.kaggle.com/datasets/anmolkumar/glove-embeddings?select=glove.6B.100d.txt) or [https://zenodo.org/records/4925376](https://zenodo.org/records/4925376).

Once downloaded, create a directory in this project's main folder called `model`, and place all 4 files there.

### Running the program:

You can run the program on the sample data by typing: 

    python train.py

into your device's terminal or command-prompt window.

As needed, update the paths to the training and test file and the model hyperparameters before running the program within the `Constants.py` file.

### Examples of running the Python program

As described above, the folder `experiments` contains outputs of running the Python program in 3 instances:

   1. Using the `bert-base-uncased` language model, on the training CSV file `./csv_data_files/train/student_answers_mini.csv` and test CSV file `./csv_data_files/test/student_answers_mini.csv`

   2. Using the `bert-base-uncased` language model, on the training CSV file `./csv_data_files/train/student_answers_mini_improved.csv` and test CSV file `./csv_data_files/test/student_answers_mini_improved.csv`. The only difference between the original and improved files is that missing student answers (e.g., when a student didn't provide any answer) were replaced with the phrase `No answer was provided`.

   3. Using the `roberta-base` language model, again on the training CSV file `./csv_data_files/train/student_answers_mini_improved.csv` and test CSV file `./csv_data_files/test/student_answers_mini_improved.csv`.

These printouts are provided in case, for whatever reason, your device struggles to run the programs provided in this repository.
